// Copyright (c) 2017, Lawrence Livermore National Security, LLC. Produced at
// the Lawrence Livermore National Laboratory. LLNL-CODE-734707. All Rights
// reserved. See files LICENSE and NOTICE for details.
//
// This file is part of CEED, a collection of benchmarks, miniapps, software
// libraries and APIs for efficient high-order finite element and spectral
// element discretizations for exascale applications. For more information and
// source code availability see http://github.com/ceed.
//
// The CEED research is supported by the Exascale Computing Project 17-SC-20-SC,
// a collaborative effort of two U.S. Department of Energy organizations (Office
// of Science and the National Nuclear Security Administration) responsible for
// the planning and preparation of a capable exascale ecosystem, including
// software, applications, hardware, advanced system engineering and early
// testbed platforms, in support of the nation's exascale computing imperative.

// *****************************************************************************
kernel void kRestrict0(const int *indices,
                       const double* uu,
                       double* vv) {
  for (int i=0; i<nelem_x_elemsize; i++; tile(TILE_SIZE)){
    if (i >= nelem_x_elemsize) continue;
    vv[i] = uu[indices[i]];
  }
}

// *****************************************************************************
kernel void kRestrict1(const int ncomp,
                       const int *indices,
                       const double* uu,
                       double* vv) {
  for (int e = 0; e < nelem; e++; tile(TILE_SIZE)){
    if (e >= nelem) continue;
    for (int d = 0; d < ncomp; d++){
      for (int i=0; i<elemsize; i++) {
        vv[i+elemsize*(d+ncomp*e)] =
          uu[indices[i+elemsize*e]+ndof*d];
      }
    }
  }
}

// *****************************************************************************
kernel void kRestrict2(const int ncomp,
                       const int *indices,
                       const double* uu,
                       double* vv) {
  for (int e = 0; e < nelem; e++; tile(TILE_SIZE)){
    if (e >= nelem) continue;
    for (int d = 0; d < ncomp; d++){
      for (int i=0; i<elemsize; i++) {
        vv[i+elemsize*(d+ncomp*e)] =
          uu[d+ncomp*indices[i+elemsize*e]];
      }
    }
  }
}

// *****************************************************************************
kernel void kRestrict3(const int *indices,
                       const double* uu,
                       double* vv) {
  //printf("\nkRestrict3");
  for (int i=0; i<nelem_x_elemsize; i++; tile(TILE_SIZE)){
    if (i >= nelem_x_elemsize) continue;
    //printf("\nv[%d]=%f",indices[i],vv[indices[i]]);
    atomicAdd(vv + indices[i], uu[i]);
    //printf("\nv[%d]+=u[%d](=%f)",indices[i],i,uu[i]);
  }
}

// *****************************************************************************
kernel void kRestrict3b(const int *tindices,
                        const int *offsets,
                        const double* uu,
                        double* vv) {
  //printf("\nkRestrict3b");
  for (int i=0; i<ndof; i++; tile(TILE_SIZE)){
    if (i >= ndof) continue;
    const int offset = offsets[i];
    const int nextOffset = offsets[i+1];
    double dofValue = 0.0;
    //printf("\nv[%d]+=",i);
    for (int j=offset; j<nextOffset; ++j){
      const int tid = tindices[j];
      dofValue += uu[tid];
      //printf("u[%d] ",tid);
    }
    vv[i] = dofValue;
  }
}

// *****************************************************************************
kernel void kRestrict4(const int ncomp,
                       const int *indices,
                       const double* uu,
                       double* vv) {
  for (int e = 0; e < nelem; e++; tile(TILE_SIZE)){
    if (e >= nelem) continue;
    for (int d = 0; d < ncomp; d++){
      for (int i=0; i<elemsize; i++) {
         atomicAdd(vv + indices[i+elemsize*e]+ndof*d, uu[i+elemsize*(d+e*ncomp)]);
      }
    }
  }
}
// *****************************************************************************
kernel void kRestrict4b(const int ncomp,
                        const int *indices,
                        const int *offsets,
                        const double* uu,
                        double* vv) {
  for (int i=0; i<ndof; i++; tile(TILE_SIZE)){
    if (i >= ndof) continue;
    const int offset = offsets[i];
    const int nextOffset = offsets[i+1];
    for (int d = 0; d < ncomp; ++d) {
      double dofValue = 0.0;
      for (int j=offset; j<nextOffset; ++j){
        const int lxs = d+indices[j]*ncomp;//ijNM(d,indices[j],ncomp,elemsize);
        dofValue += uu[lxs];
      }
      const int gxs = d+i*ncomp;//ijNM(d,i,ncomp,ndof);
      vv[gxs] = dofValue;
    }
  }
}

// *****************************************************************************
kernel void kRestrict5(const int ncomp,
                       const int *indices,
                       const double* uu,
                       double* vv) {
  for (int e = 0; e < nelem; e++; tile(TILE_SIZE)){
    if (e >= nelem) continue;
    for (int d = 0; d < ncomp; d++){
      for (int i=0; i<elemsize; i++) {
         atomicAdd(vv + d+ncomp*indices[i+elemsize*e], uu[i+elemsize*(d+e*ncomp)]);
      }
    }
  }
}

// *****************************************************************************
kernel void kRestrict5b(const int ncomp,
                        const int *indices,
                        const int *offsets,
                        const double* uu,
                        double* vv) {
  for (int e = 0; e < nelem; e++; tile(TILE_SIZE)){
    if (e >= nelem) continue;
    for (int d = 0; d < ncomp; d++){
      for (int i=0; i<elemsize; i++) {
        atomicAdd(vv + d+ncomp*indices[i+elemsize*e], uu[i+elemsize*(d+e*ncomp)]);
      }
    }
  }
}
